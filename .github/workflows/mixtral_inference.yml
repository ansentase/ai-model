name: HuggingFace Inference Workflow

on:
  workflow_dispatch:
    inputs:
      prompt:
        description: "Enter a text prompt"
        required: true
        default: "Explain the evolution of human intelligence in deep detail."

jobs:
  run-inference:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install openai

      - name: Create and run inference script
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
          PROMPT_INPUT: ${{ github.event.inputs.prompt }}
        run: |
          cat <<EOF > script.py
          import os
          from openai import OpenAI

          HF_TOKEN = os.getenv("HF_API_TOKEN")
          if not HF_TOKEN:
              raise ValueError("HF_API_TOKEN not set.")

          prompt = os.getenv("PROMPT_INPUT", "Give me 1000 words script on productivity")

          client = OpenAI(
              base_url="https://api-inference.huggingface.co/v1/",
              api_key=HF_TOKEN,
          )

          system_message = "You are a detailed and insightful assistant."

          response = client.chat.completions.create(
              model="meta-llama/Llama-3.3-70B-Instruct",
              max_tokens=2048,
              temperature=0.7,
              top_p=0.95,
              frequency_penalty=0.0,
              messages=[
                  {"role": "system", "content": system_message},
                  {"role": "user", "content": prompt}
              ],
              stream=False
          )

          output = response.choices[0].message.content.strip()
          print("=== MODEL RESPONSE ===")
          print(output)
          EOF

          python script.py
