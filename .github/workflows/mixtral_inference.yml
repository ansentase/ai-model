name: HuggingFace Inference Workflow

on:
  workflow_dispatch:

jobs:
  generate-text:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repo
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: pip install openai

      - name: Run Hugging Face inference
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
        run: |
          import os
          from openai import OpenAI

          prompt = "give me 1000 words script on productivity"
          hf_token = os.getenv("HF_API_TOKEN")

          client = OpenAI(
              base_url="https://api-inference.huggingface.co/v1/",
              api_key=hf_token,
          )

          messages = [
              {"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": prompt},
          ]

          response = ""
          for chunk in client.chat.completions.create(
              model="meta-llama/Llama-3.3-70B-Instruct",
              max_tokens=4096,
              stream=True,
              temperature=0.7,
              top_p=0.95,
              frequency_penalty=0.0,
              seed=None,
              messages=messages,
          ):
              token = chunk.choices[0].delta.content
              if token:
                  print(token, end="", flush=True)
                  response += token
