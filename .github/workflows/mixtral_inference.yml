name: HuggingFace Inference Workflow

on:
  workflow_dispatch:

jobs:
  generate-text:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install requests

      - name: Run Hugging Face Inference
        env:
          HF_API_TOKEN: ${{ secrets.HF_API_TOKEN }}
        run: |
          echo 'import os
import requests

# Set up Hugging Face API details
API_URL = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct"
headers = {
    "Authorization": f"Bearer {os.getenv('HF_API_TOKEN')}",
}

# Your prompt
prompt = "give me 1000 words script on productivity"

# Prepare the request payload
data = {
    "inputs": prompt,
    "parameters": {
        "max_length": 1000,  # Limit the output to 1000 words
        "temperature": 0.7,
        "top_p": 0.95,
        "frequency_penalty": 0.0,
    },
}

# Make the request to Hugging Face model
response = requests.post(API_URL, headers=headers, json=data)

# Handle response
if response.status_code == 200:
    output = response.json()[0]["generated_text"]
    print(f"Generated text: {output}")
else:
    print(f"Error: {response.status_code} - {response.text}")
' > script.py

          python script.py
